{
    "overall": {
        "accuracy": 0.5859722585710547,
        "macro avg": {
            "precision": 0.6365680306980416,
            "recall": 0.5455517336449895,
            "f1-score": 0.5476022835892322,
            "support": 3821
        },
        "weighted avg": {
            "precision": 0.6813009145238521,
            "recall": 0.5859722585710547,
            "f1-score": 0.6013677755138331,
            "support": 3821
        }
    },
    "Angry": {
        "precision": 0.58,
        "recall": 0.1336405529953917,
        "f1-score": 0.21722846441947566,
        "accuracy": 0.1336405529953917,
        "support": 434,
        "True Positives": 58,
        "False Positives": 42,
        "False Negatives": 376,
        "True Negatives": 3345
    },
    "Disgust": {
        "precision": 0.3062953995157385,
        "recall": 0.6200980392156863,
        "f1-score": 0.4100486223662885,
        "accuracy": 0.6200980392156863,
        "support": 408,
        "True Positives": 253,
        "False Positives": 573,
        "False Negatives": 155,
        "True Negatives": 2840
    },
    "Fear": {
        "precision": 0.8333333333333334,
        "recall": 0.3583061889250814,
        "f1-score": 0.5011389521640092,
        "accuracy": 0.3583061889250814,
        "support": 307,
        "True Positives": 110,
        "False Positives": 22,
        "False Negatives": 197,
        "True Negatives": 3492
    },
    "Happiness": {
        "precision": 0.8813387423935092,
        "recall": 0.7333333333333333,
        "f1-score": 0.8005527406725012,
        "accuracy": 0.7333333333333333,
        "support": 1185,
        "True Positives": 869,
        "False Positives": 117,
        "False Negatives": 316,
        "True Negatives": 2519
    },
    "Neutral": {
        "precision": 0.6280587275693311,
        "recall": 0.5661764705882353,
        "f1-score": 0.5955143078112916,
        "accuracy": 0.5661764705882353,
        "support": 680,
        "True Positives": 385,
        "False Positives": 228,
        "False Negatives": 295,
        "True Negatives": 2913
    },
    "Sadness": {
        "precision": 0.6415841584158416,
        "recall": 0.6778242677824268,
        "f1-score": 0.659206510681587,
        "accuracy": 0.6778242677824268,
        "support": 478,
        "True Positives": 324,
        "False Positives": 181,
        "False Negatives": 154,
        "True Negatives": 3162
    },
    "Surprise": {
        "precision": 0.5853658536585366,
        "recall": 0.729483282674772,
        "f1-score": 0.6495263870094723,
        "accuracy": 0.729483282674772,
        "support": 329,
        "True Positives": 240,
        "False Positives": 170,
        "False Negatives": 89,
        "True Negatives": 3322
    }
}