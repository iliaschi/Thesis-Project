{
    "overall": {
        "accuracy": 0.46061240512954726,
        "macro avg": {
            "precision": 0.5556864239000775,
            "recall": 0.4843062013413591,
            "f1-score": 0.46703369408493833,
            "support": 3821
        },
        "weighted avg": {
            "precision": 0.6387774282403539,
            "recall": 0.46061240512954726,
            "f1-score": 0.48778174216231324,
            "support": 3821
        }
    },
    "Angry": {
        "precision": 0.5338983050847458,
        "recall": 0.14516129032258066,
        "f1-score": 0.22826086956521743,
        "accuracy": 0.14516129032258066,
        "support": 434,
        "True Positives": 63,
        "False Positives": 55,
        "False Negatives": 371,
        "True Negatives": 3332
    },
    "Disgust": {
        "precision": 0.26512968299711814,
        "recall": 0.6764705882352942,
        "f1-score": 0.38095238095238093,
        "accuracy": 0.6764705882352942,
        "support": 408,
        "True Positives": 276,
        "False Positives": 765,
        "False Negatives": 132,
        "True Negatives": 2648
    },
    "Fear": {
        "precision": 0.5756302521008403,
        "recall": 0.44625407166123776,
        "f1-score": 0.5027522935779816,
        "accuracy": 0.44625407166123776,
        "support": 307,
        "True Positives": 137,
        "False Positives": 101,
        "False Negatives": 170,
        "True Negatives": 3413
    },
    "Happiness": {
        "precision": 0.9565217391304348,
        "recall": 0.4084388185654008,
        "f1-score": 0.5724423418095801,
        "accuracy": 0.4084388185654008,
        "support": 1185,
        "True Positives": 484,
        "False Positives": 22,
        "False Negatives": 701,
        "True Negatives": 2614
    },
    "Neutral": {
        "precision": 0.5107142857142857,
        "recall": 0.42058823529411765,
        "f1-score": 0.4612903225806451,
        "accuracy": 0.42058823529411765,
        "support": 680,
        "True Positives": 286,
        "False Positives": 274,
        "False Negatives": 394,
        "True Negatives": 2867
    },
    "Sadness": {
        "precision": 0.662004662004662,
        "recall": 0.5941422594142259,
        "f1-score": 0.6262403528114663,
        "accuracy": 0.5941422594142259,
        "support": 478,
        "True Positives": 284,
        "False Positives": 145,
        "False Negatives": 194,
        "True Negatives": 3198
    },
    "Surprise": {
        "precision": 0.3859060402684564,
        "recall": 0.6990881458966566,
        "f1-score": 0.49729729729729727,
        "accuracy": 0.6990881458966566,
        "support": 329,
        "True Positives": 230,
        "False Positives": 366,
        "False Negatives": 99,
        "True Negatives": 3126
    }
}