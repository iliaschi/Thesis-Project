{
    "overall": {
        "accuracy": 0.46061240512954726,
        "macro avg": {
            "precision": 0.4765849111752538,
            "recall": 0.42095522223200493,
            "f1-score": 0.40301758477013094,
            "support": 3821
        },
        "weighted avg": {
            "precision": 0.6273665754651506,
            "recall": 0.46061240512954726,
            "f1-score": 0.48319549054703737,
            "support": 3821
        }
    },
    "Angry": {
        "precision": 0.5283018867924528,
        "recall": 0.12903225806451613,
        "f1-score": 0.2074074074074074,
        "accuracy": 0.12903225806451613,
        "support": 434,
        "True Positives": 56,
        "False Positives": 50,
        "False Negatives": 378,
        "True Negatives": 3337
    },
    "Contempt": {
        "precision": 0.0,
        "recall": 0.0,
        "f1-score": 0.0,
        "accuracy": 0.0,
        "support": 0,
        "True Positives": 0,
        "False Positives": 314,
        "False Negatives": 0,
        "True Negatives": 3507
    },
    "Disgust": {
        "precision": 0.27037773359840955,
        "recall": 0.6666666666666666,
        "f1-score": 0.38472418670438474,
        "accuracy": 0.6666666666666666,
        "support": 408,
        "True Positives": 272,
        "False Positives": 734,
        "False Negatives": 136,
        "True Negatives": 2679
    },
    "Fear": {
        "precision": 0.5688888888888889,
        "recall": 0.4169381107491857,
        "f1-score": 0.48120300751879697,
        "accuracy": 0.4169381107491857,
        "support": 307,
        "True Positives": 128,
        "False Positives": 97,
        "False Negatives": 179,
        "True Negatives": 3417
    },
    "Happiness": {
        "precision": 0.9571150097465887,
        "recall": 0.41434599156118146,
        "f1-score": 0.5783274440518257,
        "accuracy": 0.41434599156118146,
        "support": 1185,
        "True Positives": 491,
        "False Positives": 22,
        "False Negatives": 694,
        "True Negatives": 2614
    },
    "Neutral": {
        "precision": 0.47877758913412566,
        "recall": 0.4147058823529412,
        "f1-score": 0.4444444444444444,
        "accuracy": 0.4147058823529412,
        "support": 680,
        "True Positives": 282,
        "False Positives": 307,
        "False Negatives": 398,
        "True Negatives": 2834
    },
    "Sadness": {
        "precision": 0.6116700201207244,
        "recall": 0.6359832635983264,
        "f1-score": 0.6235897435897437,
        "accuracy": 0.6359832635983264,
        "support": 478,
        "True Positives": 304,
        "False Positives": 193,
        "False Negatives": 174,
        "True Negatives": 3150
    },
    "Surprise": {
        "precision": 0.3975481611208406,
        "recall": 0.6899696048632219,
        "f1-score": 0.5044444444444445,
        "accuracy": 0.6899696048632219,
        "support": 329,
        "True Positives": 227,
        "False Positives": 344,
        "False Negatives": 102,
        "True Negatives": 3148
    }
}